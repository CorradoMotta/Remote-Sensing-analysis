{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic download and processing of OLCI satellite images using SNAP on python\n",
    "*Author:* Corrado Motta - corradomotta92@gmail.com\n",
    "\n",
    "This notebook is intended to cover the following steps by making use of python code and libraries:\n",
    "- Automatically downloading multiple OLCI Sentinel-3 satellite images including chl-a and TSM concentration with a number of configuration items (e.g. time window, clouds coverage, area of interest, type of product, etc.).\n",
    "- Checking the integrity of downloaded data and plot them.\n",
    "- Processing the downloaded images (chl and tsm analysis supported by now):\n",
    "    - Subsetting the images to the area of interest.\n",
    "    - Masking the unvalid data.\n",
    "    - Performing temporal aggregation of some kind on a band of interest. Types of aggregation available:\n",
    "        - Average\n",
    "        - Average with Outliers\n",
    "        - Min and Max\n",
    "    - Save the final results (either in .nc or .dim) and show them on the notebook as well.\n",
    "- Extracting pixel values by given coordinates points and save them on an excel sheet.\n",
    "\n",
    "Further this notebook is divided into sections and sections into blocks with informative text to facilitate the understanding of each part. More information are contained into the handbook provided as a PDF as part of this internship. As this is a new script run a limited number of times, it can contains bug/errors. Contact me for any error or information you may need.\n",
    "\n",
    "**NOTE!**\n",
    "Before running this script, you need to make sure the global environmental variables for OLCI Sentinel-3 data are correctly set. To do that you need to open SNAP toolbox:\n",
    "\n",
    "- Open SNAP. Click on Tools > Options > S3TBX and check the box shown in figure. More information in the handbook.\n",
    "- Uncheck the other boxes if you do not need them.\n",
    "\n",
    "<img src=\"images/Orthorectified.PNG\" width=\"600\"/>\n",
    "\n",
    "You will also need to create an account at EUMETSAT. More information are stored in the handbook.\n",
    "\n",
    "## Setting the configuration file\n",
    "This notebook comes with a configuration file named `OLCI_configuration.ini`. There, it is possible to set all parameters used in this script. You can see four different sections in the file:\n",
    "1. **account_options** Used to store your account credentials to EUMETSAT.\n",
    "2. **storage_options** Where you want your data and results to be stored.\n",
    "3. **download_options** Set of options to download the OLCI imagery.\n",
    "4. **flag_options** Set of flags to download the OLCI imagery.\n",
    "5. **sentinel3_request_options** Specific set of options for Sentinel-3 imagery (for example the product type).\n",
    "6. **aggr_request_options** Set of options to perform the temporal aggregation and the pixel values extraction.\n",
    "\n",
    "More information are contained under each section and before each parameter directly in the configuration file.\n",
    "Before run this notebook, make sure to set the configurable parameters as you like, the default configuration is for one of the scenario described in the handbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic download of Sentinel-3 OLCI imagery \n",
    "In order to download Sentinel-3 OLCI imagery we need to access EUMETSAT data. The best way to do that is to make use of the existing python resources developed by EUMETSAT itself and provided in their GitLab repository: https://gitlab.eumetsat.int/eumetlab.  Under \"cross-cutting tool\" a Python-based Sentinel satellite data downloader is already available. The readme file contains all the needed information to run the script properly. \n",
    "\n",
    "In this notebook you just need to run the next code blocks. First of all let's import the basic packages needed at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# miscellaneous OS interfaces\n",
    "import os \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import warnings\n",
    "import configparser\n",
    "import ctypes\n",
    "\n",
    "# to unzip the files\n",
    "from zipfile import ZipFile\n",
    "\n",
    "#constants\n",
    "MB_OK = 0x0\n",
    "MB_YESNO = 0x04\n",
    "ICON_STOP = 0x10\n",
    "ICON_EXLAIM=0x30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we start to read the configurable parameters we need to perform the download. After, we check that all folders exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the ini file and store the values into variables\n",
    "read_config = configparser.ConfigParser()\n",
    "read_config.read(\"OLCI_configuration.ini\")\n",
    "\n",
    "data_path = read_config.get(\"storage_options\", \"output_root_directory\")\n",
    "result = read_config.get(\"storage_options\", \"result_root_directory\")\n",
    "file_manifest = \"xfdumanifest.xml\"\n",
    "\n",
    "# get specified bands if any\n",
    "if(read_config.get(\"download_options\",\"get_specified_bands\") == \"True\"):\n",
    "    # read directly into a list\n",
    "    spec_band = read_config.get(\"download_options\", \"specified_bands\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that folders exists or create them.\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "if not os.path.exists(result):\n",
    "    os.makedirs(result)\n",
    "\n",
    "# check if the path is empty\n",
    "if(os.listdir(data_path) and read_config.get(\"download_options\",\"run_download\") == \"True\"):\n",
    "    MessageBox = ctypes.windll.user32.MessageBoxA\n",
    "    chs = MessageBox(None, 'Directory is not empty! please not that the script will remove all not zipped files contained in this directory. Do you want to proceed?', 'Warning', MB_YESNO | ICON_EXLAIM)\n",
    "    if(chs == 7): raise SystemExit(\"Directory not empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous conditions are satisfied, we are ready to run the downloader script. However if the `run_download` option is set to false, this part will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(read_config.get(\"download_options\",\"run_download\") == \"True\"):\n",
    "    %run -i Universal_Sentinel_Downloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check what we have in the folder now\n",
    "if(len(listdir(data_path)) ==0): raise SystemExit(\"The directory is empty!\")\n",
    "print('number of items in data path: {0}'.format(len(listdir(data_path))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data integrity check\n",
    "Great! We downloaded our images in the predefined folder. The next step is to unzip the files (if needed) and check their integrity. We first define a couple of simple function to help with the checks and the unzip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to unzip file\n",
    "def unzip_data(filename):\n",
    "    \"\"\" Function to unzip data and remove the zip file.\n",
    "    \n",
    "    Parameters:\n",
    "    filename (string): file name to be unzipped.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print('Unzipping file {0}'.format(filename))\n",
    "        with ZipFile(filename, \"r\") as zipObj:\n",
    "            zipObj.extractall(data_path)\n",
    "            # clear up the zip file\n",
    "        os.remove(filename)\n",
    "        print('done!')\n",
    "    except:\n",
    "        print(\"Failed to unzip....\")\n",
    "        \n",
    "def integrity_check(in_dir, in_ref):\n",
    "    \"\"\" Function to check that a directory contains at least all file names given by a list.\n",
    "    \n",
    "    Parameters:\n",
    "    in_dir (string): path to folder.\n",
    "    in_ref (string): list of file names to check.\n",
    "    \n",
    "    Returns:\n",
    "    A boolean indicating the result.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for name in in_dir:\n",
    "        if name in in_ref:\n",
    "            count +=1\n",
    "    if count == len(in_ref):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to performs the checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define a list that will contain all valid paths\n",
    "path_list = []\n",
    "unvalid_list = []\n",
    "\n",
    "# If we have new data as entire products, we unzip the\n",
    "for f in listdir(data_path):\n",
    "    if(f.endswith(\".zip\")):\n",
    "        path_list.append(join(data_path, f))\n",
    "\n",
    "if(not path_list):\n",
    "    print(\"No zip files in the directory.\")\n",
    "else:\n",
    "    # unzip routine\n",
    "    map(unzip_data, path_list)\n",
    "\n",
    "# If we downloaded new data with only specific bands, let's check that the directories contains all the files that are needed and add them to path_list\n",
    "if(read_config.get(\"download_options\",\"get_specified_bands\") == \"True\"):\n",
    "    unvalid_list = []\n",
    "    for f in listdir(data_path):\n",
    "        fname = join(data_path,f)\n",
    "        dirs = listdir(fname)\n",
    "        if (integrity_check(dirs, spec_band)):\n",
    "            path_list.append(fname)\n",
    "        else:\n",
    "            unvalid_list.append(fname)\n",
    "            \n",
    "if(unvalid_list):\n",
    "    print(\"{0} product(s) over {1} are invalid:\".format(len(unvalid_list), len(listdir(data_path))))\n",
    "    for number, letter in enumerate(unvalid_list):\n",
    "        print(\"-- \" + str(number + 1) + \": \" + letter)\n",
    "\n",
    "elif(path_list):\n",
    "    print(\"All items are valid!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check path_list\n",
    "#path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting one of the image\n",
    "Let's take one of the downloaded product (**default**: the first in the list) and plot it to make sure the area downloaded is the correct one. Here we start to make use of the SNAP functionalities, directly in python. We are going to import one of the band (**default**: the chl-a one) and print its B/N image representation.\n",
    "\n",
    "In python the snappy package is available and based on SNAP toolbox (which is implemented on JAVA). Check: https://senbox.atlassian.net/wiki/spaces/SNAP/pages/19300362/How+to+use+the+SNAP+API+from+Python. More information can be found in the handbook.\n",
    "Once installed (unfortunately snappy supports only python 2.7, 3.3 or 3.4 so python version should be set accordingly), you can use the same functions of SNAP here in python. \n",
    "Let's start by importing the snappy functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib and numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# import math for small math operations\n",
    "import math\n",
    "\n",
    "# importing snap functions\n",
    "import snappy\n",
    "from snappy import ProductIO, WKTReader\n",
    "from snappy import jpy\n",
    "from snappy import GPF\n",
    "from snappy import HashMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use snappy to simply import the product and select the band. Then a numpy array will be filled with the band values and plotted to form our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ProductIO.readProduct(os.path.join(path_list[0],file_manifest))\n",
    "band = df.getBand('CHL_NN') # Assign Band to a variable\n",
    "w = df.getSceneRasterWidth() # Get Band Width\n",
    "h = df.getSceneRasterHeight() # Get Band Height\n",
    "# Create an empty array\n",
    "band_data = np.zeros(w * h, np.float32)\n",
    "# Populate array with pixel value\n",
    "band.readPixels(0, 0, w, h, band_data) \n",
    "# Reshape\n",
    "band_data.shape = h, w\n",
    "# log10\n",
    "output_array = np.log10(band_data)\n",
    "# Plot the band  \n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "plt.imshow(output_array, cmap = plt.cm.binary)\n",
    "plt.title(\"{0} B/N\".format(df.getDescription()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Aggregation\n",
    "Now we can use snappy to aggregate the satellite images and create averages. Note that this section works also indipendently from the automatic download. You can manually download Sentinel-3 data and store them in your root directory and change the `run_download` flag to false in the configuration file.\n",
    "\n",
    "First we read the configuration file again and we save the parameters for the aggregation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all needed configuration data from the ini file\n",
    "read_config = configparser.ConfigParser()\n",
    "read_config.read(\"OLCI_configuration.ini\")\n",
    "\n",
    "band_name = read_config.get(\"aggr_request_options\", \"aggr_band_name\").split(\",\")\n",
    "aggr_type = read_config.get(\"aggr_request_options\", \"aggr_type\").split(\",\")\n",
    "file_name = read_config.get(\"aggr_request_options\", \"aggr_file_name\")\n",
    "file_format = read_config.get(\"aggr_request_options\", \"aggr_file_format\")\n",
    "mask_expr = read_config.get(\"aggr_request_options\", \"aggr_mask_expression\")\n",
    "super_sampl = read_config.get(\"aggr_request_options\", \"aggr_super_sampling\")\n",
    "\n",
    "wkt = read_config.get(\"aggr_request_options\", \"aggr_wkt\")\n",
    "resolution = float(read_config.get(\"aggr_request_options\", \"aggr_resolution\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then proceed with the aggregation. We will use the binning function from SNAP. First we define some simple functions. The description of each function is also provided under them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_rows(res_km):\n",
    "    \"\"\" Function to compute the number of rows from a resolution given in km. Taken from SNAP implementation.\n",
    "   \n",
    "    Parameters:\n",
    "    res_km (float): resolution in kilometers.\n",
    "    \n",
    "    Returns:\n",
    "    Resolution in rows.\n",
    "    \"\"\"\n",
    "    RE = 6378.145;\n",
    "    num_rows =  (int) ((RE * math.pi) / res_km) + 1\n",
    "    return num_rows if num_rows % 2 == 0 else num_rows + 1\n",
    "\n",
    "\n",
    "def combine_paths(path):\n",
    "    \"\"\" Function to combine all paths together and separated by a comma, for the SNAP aggregation.\n",
    "    \n",
    "    Parameters:\n",
    "    path (list): A list of paths to be combined.\n",
    "    \n",
    "    Returns:\n",
    "    A string with all paths combined together.\n",
    "    \"\"\"\n",
    "    path_combined = \"\"\n",
    "    for f in path:\n",
    "        if(path_combined):\n",
    "            path_combined +=\",\"\n",
    "        path_combined += join(f,file_manifest)\n",
    "    return path_combined\n",
    "\n",
    "def band_to_olci_name(name):\n",
    "    \"\"\" Function to convert the input name from the configuration file into the conventional OLCI name needed to perform SNAP operations.\n",
    "    \n",
    "    Parameters:\n",
    "    name (string): the name as specified in the configuration file.\n",
    "    \n",
    "    Returns:\n",
    "    The OLCI name.\n",
    "    \"\"\"\n",
    "    olci_name = ''\n",
    "    if(name == \"chl\"): olci_name = 'CHL_NN'\n",
    "    elif(name == \"tsm\"): olci_name = 'TSM_NN'\n",
    "    else: raise SystemExit(\"band name not assigned or wrong: \", name)\n",
    "    return olci_name\n",
    "\n",
    "def combine_file_format(name, extension):\n",
    "    \"\"\" Function to correctly add extensions to filename\n",
    "    \n",
    "    Parameters:\n",
    "    name (string): the name as specified in the configuration file.\n",
    "    extension (string): the format as as specified in the configuration file.\n",
    "    \n",
    "    Returns:\n",
    "    The name with the extension\n",
    "    \"\"\"\n",
    "    ext = ''\n",
    "    if(extension == \"BEAM-DIMAP\"): ext = '.dim'\n",
    "    elif(extension == \"NetCDF4-CF\" or extension == \"NetCDF4-BEAM\"): ext = '.nc'\n",
    "    else: raise SystemExit(\"output name or extension not assigned or wrong: \", ext)\n",
    "    return (name + ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, based on the configuration file, we set all the parameters for the snappy binning function. Within this function we are able to:\n",
    "- Select the product to be aggregated (chl or tsm or both).\n",
    "- Create several different aggregators (e.g. AVG, AVG_OUTLIER, MIN_MAX).\n",
    "- Mask out clouds, invalid pixels and where the algorithm might have failed.\n",
    "- Create a subset region where to operate (**default:** Swedish NW coast).\n",
    "- Set the resolution in km.\n",
    "- Use super-sampling.\n",
    "- Save it in different formats (**default:** BEAM-DIMAP. NETCDF4 also available).\n",
    "\n",
    "The next code block, set all parameters in the snappy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the target bands\n",
    "array_dim = len(aggr_type) * len(band_name)\n",
    "aggregators = snappy.jpy.array('org.esa.snap.binning.AggregatorConfig',array_dim)\n",
    "\n",
    "# creates and fill aggregators for all defined bands\n",
    "count_in = 0 \n",
    "for aggregator in aggr_type:\n",
    "    for band in band_name:\n",
    "        olci_name = band_to_olci_name(band)\n",
    "        if(aggregator == 'AVG_OUTLIER'):\n",
    "            aggregator_config_out = snappy.jpy.get_type('org.esa.snap.binning.aggregators.AggregatorAverageOutlierAware$Config')\n",
    "            agg_constr = aggregator_config_out(olci_name, olci_name + \"_OUTLIER\", 1.0)\n",
    "        elif(aggregator == 'AVG'):\n",
    "            aggregator_config_avg = snappy.jpy.get_type('org.esa.snap.binning.aggregators.AggregatorAverage$Config')\n",
    "            agg_constr = aggregator_config_avg(olci_name, olci_name, 1.0, False, False)\n",
    "        elif(aggregator == 'MIN_MAX'):\n",
    "            aggregator_config_min_max = snappy.jpy.get_type('org.esa.snap.binning.aggregators.AggregatorMinMax$Config')\n",
    "            agg_constr = aggregator_config_min_max(olci_name, olci_name)\n",
    "        aggregators[count_in] = agg_constr\n",
    "        print('aggregators filled at position: ', (count_in))\n",
    "        count_in += 1\n",
    "\n",
    "# creating the hashmap to store the parameters\n",
    "HashMap = snappy.jpy.get_type('java.util.HashMap')\n",
    "parameters = HashMap()\n",
    "\n",
    "# set file name and format\n",
    "dir_out = combine_file_format(join(result,file_name),file_format)\n",
    "parameters.put('outputFile', dir_out)\n",
    "parameters.put('outputFormat', file_format)\n",
    "\n",
    "# apply masks to computation\n",
    "parameters.put('maskExpr', mask_expr)\n",
    "\n",
    "# number of rows\n",
    "num_rows = compute_num_rows(resolution)\n",
    "parameters.put('numRows', num_rows) \n",
    "\n",
    "# super sampling\n",
    "parameters.put('superSampling', super_sampl) \n",
    "\n",
    "# aggregators list\n",
    "parameters.put('aggregators', aggregators)\n",
    "\n",
    "# Region to clip the aggregation on\n",
    "geom = WKTReader().read(wkt)\n",
    "parameters.put('region', geom)\n",
    "\n",
    "# Source product path \n",
    "path_combined = combine_paths(path_list)\n",
    "parameters.put('sourceProductPaths', path_combined)\n",
    "\n",
    "print(\"\\n------------------------------------------------\"\n",
    "      \"\\nAll parameters set:\" \n",
    "      \"\\n---Bands: {0} \\n---Aggregator methods: {1} \\n---Output file: {2}\\n---Output Format: {3} \\n---Resolution in rows: {4}\"\n",
    "      \"\\n---Super-sampling: {5}\\n---Mask expression: {6},\\n---Region: {7}\".format(band_name,aggr_type,dir_out,file_format, num_rows, super_sampl, mask_expr, geom))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create results and store it into our dedicated folder.\n",
    "\n",
    "**NOTE:** if the file name (`aggr_file_name` in the .ini file ) is the same of the previous run, it will override."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create results\n",
    "aggr_product = snappy.GPF.createProduct('Binning', parameters) #to be used with product paths specified in the parameters hashmap\n",
    "\n",
    "print(\"results stored in: {0}\".format(dir_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open and check the generated product\n",
    "The product is now saved in the folder and can be open in SNAP toolbox. Let's have a look directly here with python by using other snappy functionalities. First let's check which bands ara available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(aggr_product.getBandNames()))\n",
    "lista = aggr_product.getBandNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`aggr_product` contains our temporal aggregation. Let's plot one of the band and check how it looks like. To plot it, we are going to use matplotlib and therefore we need to convert our band to a numpy array. To have a proper look into the created products, it is necessary to open it on SNAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the band to be shown in the map. Default value is the first target band in the generated product\n",
    "band_to_print = ''\n",
    "for l in lista:\n",
    "    if olci_name in l:\n",
    "        band_to_print = l\n",
    "        break;\n",
    "print(band_to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your band to print manually\n",
    "# band_to_print = 'CHL_NN_OUTLIER_mean'\n",
    "# Assign Band to a variable\n",
    "band = aggr_product.getBand(band_to_print)\n",
    "\n",
    "# Get band width and height\n",
    "w = aggr_product.getSceneRasterWidth() \n",
    "h = aggr_product.getSceneRasterHeight()\n",
    "\n",
    "# Create an empty array\n",
    "band_data = np.zeros(w * h, np.float32)\n",
    "\n",
    "# Populate array with pixel value\n",
    "band.readPixels(0, 0, w, h, band_data) \n",
    "\n",
    "# Reshape the numpy array\n",
    "band_data.shape = h, w\n",
    "\n",
    "# do log10\n",
    "output_array = np.log10(band_data)\n",
    "\n",
    "# Plot the band  \n",
    "fig, ax = plt.subplots(figsize=(18,10))\n",
    "pcm = ax.pcolormesh(output_array, vmin=-1.5, vmax=1.3, cmap=plt.cm.jet)\n",
    "ax.invert_yaxis()\n",
    "#im = ax.imshow(output_array, cmap = plt.cm.jet)\n",
    "plt.title(band_to_print + \" temporal aggregation (log10)\")\n",
    "fig.colorbar(pcm, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract pixel values from a list of coordinates\n",
    "In this section we finally extract the values of pixels from the generated products. The list of stations should be a .csv or .txt file with tabular separated values. Three columns are needed at least: _Name_, _Lon_ and _Lat_. The following table is given as an example:\n",
    "\n",
    "|Name       |Lon          |Lat          |\n",
    "| ---       | ---         | ---         |\n",
    "| Station_1 | 10.92391304 | 58.92606899 |\n",
    "| Station_2 | 10.92660798 | 58.71586418 |\n",
    "| Station_3 | 11.06674452 | 58.78054258 |\n",
    "| Station_4 | 10.83767517 | 58.83983112 |\n",
    "\n",
    "The file path is specified in the configuration file and the result will be stored in the same directory as the main product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the ini file\n",
    "if(read_config.get(\"aggr_request_options\",\"aggr_pixel_extraction\") == \"True\"):\n",
    "    coor_path = read_config.get(\"aggr_request_options\", \"aggr_extraction_file_path\")\n",
    "    coor_prefix = read_config.get(\"aggr_request_options\", \"aggr_extraction_file_prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(read_config.get(\"aggr_request_options\",\"aggr_pixel_extraction\") == \"True\"):\n",
    "    # creating the hashmap to store the parameters\n",
    "    HashMap = snappy.jpy.get_type('java.util.HashMap')\n",
    "    parameters = HashMap()\n",
    "\n",
    "    # set parameters for exporting values\n",
    "    exportBands = True\n",
    "    parameters.put('exportBands', exportBands)\n",
    "\n",
    "    exportTiePoints = False\n",
    "    parameters.put('exportTiePoints', exportTiePoints)\n",
    "\n",
    "    exportMasks = False\n",
    "    parameters.put('exportMasks', exportMasks)\n",
    "    \n",
    "    # set parameters for import and export\n",
    "    parameters.put('coordinatesFile', coor_path)\n",
    "    parameters.put('outputDir', result)\n",
    "    parameters.put('outputFilePrefix', coor_prefix)\n",
    "    \n",
    "    # run the method\n",
    "    GPF.createProduct('PixEx', parameters, aggr_product)\n",
    "    \n",
    "    print(\"Pixel values extracted and stored in {0}\".format(join(result,coor_prefix)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude we can use the package Pandas to open the result txt file and print some of the values on the notebook as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the whole name\n",
    "if(read_config.get(\"aggr_request_options\",\"aggr_pixel_extraction\") == \"True\"):\n",
    "    extraction_file = ''\n",
    "    for f in listdir(result):\n",
    "        if(f.startswith(coor_prefix) and f.endswith(\"measurements.txt\")):\n",
    "            extraction_file=join(result,f)\n",
    "            \n",
    "    # print the first 5 rows of the file\n",
    "    if(extraction_file):\n",
    "        data = pd.read_csv(extraction_file, skiprows=[0,1,2,3,4,5],delimiter=\"\\t\")\n",
    "        print(data.head())\n",
    "        # save it to csv        \n",
    "        data.to_csv(join(result,coor_prefix+ \".csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the data with bars. Note that the band is selected randomly. You may want to specify your one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your band to print manually\n",
    "# band_to_print = 'CHL_NN_OUTLIER_mean'\n",
    "\n",
    "if(read_config.get(\"aggr_request_options\",\"aggr_pixel_extraction\") == \"True\"):\n",
    "    data.plot( x='Name', y=band_to_print, kind='bar', color = 'green')  \n",
    "    plt.rcParams[\"figure.figsize\"] = [10, 10]\n",
    "    plt.title(band_to_print + \" for given points\")\n",
    "    plt.xticks(rotation=30, horizontalalignment=\"center\")\n",
    "    plt.xlabel(\"Points Name\")\n",
    "    plt.ylabel(\"Concentration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
